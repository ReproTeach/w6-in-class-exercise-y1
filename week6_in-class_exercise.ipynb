{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6130c2-3a8c-4401-ae9b-c8cc743a074b",
   "metadata": {},
   "source": [
    "# Week 4 Exercise (group): Exploratory Data Analysis on Social Media Data\n",
    "\n",
    "- Yuenji Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a40a3-9ea2-4d3f-a8c5-2d98336ec4bc",
   "metadata": {},
   "source": [
    "## 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8308e587-737b-44aa-b3ee-de8542e973f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages here\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e419f3-fa12-427a-acef-9ed36c64fe5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4a085-74e8-4093-85e4-9a2f8008bf04",
   "metadata": {},
   "source": [
    "## 2. Read the data\n",
    "\n",
    "The data is called `tweets.csv` in the same folder. More information about the data see [here](https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media)\n",
    "\n",
    "The main column you will be working with is `post_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10a6b2a-52c4-4409-ba19-f199bff034b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    20000 non-null  int64 \n",
      " 1   post_id       20000 non-null  int64 \n",
      " 2   post_created  20000 non-null  object\n",
      " 3   post_text     20000 non-null  object\n",
      " 4   user_id       20000 non-null  int64 \n",
      " 5   followers     20000 non-null  int64 \n",
      " 6   friends       20000 non-null  int64 \n",
      " 7   favourites    20000 non-null  int64 \n",
      " 8   statuses      20000 non-null  int64 \n",
      " 9   retweets      20000 non-null  int64 \n",
      " 10  label         20000 non-null  int64 \n",
      "dtypes: int64(9), object(2)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# df = \n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "\n",
    "# explore the data characteristic using `df.describe()` or `df.info()`\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13566cfc-ca98-4a22-89a0-c0774944d289",
   "metadata": {},
   "source": [
    "## 3. Extract emojis\n",
    "\n",
    "Use `emoji` package to extract emojis and put them into a new column called `emojis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8287d832-ba9f-45e8-9b9e-7583d8384af1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             post_id                    post_created  \\\n",
      "0           0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n",
      "1           1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n",
      "2           2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n",
      "3           3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n",
      "4           4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n",
      "\n",
      "                                           post_text     user_id  followers  \\\n",
      "0  It's just over 2 years since I was diagnosed w...  1013187241         84   \n",
      "1  It's Sunday, I need a break, so I'm planning t...  1013187241         84   \n",
      "2  Awake but tired. I need to sleep but my brain ...  1013187241         84   \n",
      "3  RT @SewHQ: #Retro bears make perfect gifts and...  1013187241         84   \n",
      "4  It’s hard to say whether packing lists are mak...  1013187241         84   \n",
      "\n",
      "   friends  favourites  statuses  retweets  label emojis  \n",
      "0      211         251       837         0      1         \n",
      "1      211         251       837         1      1         \n",
      "2      211         251       837         0      1         \n",
      "3      211         251       837         2      1     ::  \n",
      "4      211         251       837         1      1         \n"
     ]
    }
   ],
   "source": [
    "# define the function\n",
    "def extract_emojis(text):\n",
    "    emoji_list = []\n",
    "    for character in text:\n",
    "        if character in emoji.emojize(\":\"):\n",
    "            emoji_list.append(emoji.emojize(character))\n",
    "    return ''.join(emoji_list)\n",
    "\n",
    "# apply the function to your dataframe\n",
    "df['emojis'] = df['post_text'].apply(extract_emojis)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d961f7-b745-4f4d-b516-68d9ef48dce2",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning using Regular Expressions \n",
    "\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove special characters\n",
    "5. Remove extra space\n",
    "\n",
    "Code can be found in [week 6 lecture 1](https://github.com/yibeichan/COMM160DS/blob/main/week_6/lecture_part1.ipynb) section `4.4 All-in-One`\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95fcfd0-8f32-4a44-93e9-dbb237e1eda2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             post_id                    post_created  \\\n",
      "0           0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n",
      "1           1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n",
      "2           2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n",
      "3           3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n",
      "4           4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n",
      "\n",
      "                                           post_text     user_id  followers  \\\n",
      "0  It's just over 2 years since I was diagnosed w...  1013187241         84   \n",
      "1  It's Sunday, I need a break, so I'm planning t...  1013187241         84   \n",
      "2  Awake but tired. I need to sleep but my brain ...  1013187241         84   \n",
      "3  RT @SewHQ: #Retro bears make perfect gifts and...  1013187241         84   \n",
      "4  It’s hard to say whether packing lists are mak...  1013187241         84   \n",
      "\n",
      "   friends  favourites  statuses  retweets  label emojis  \\\n",
      "0      211         251       837         0      1          \n",
      "1      211         251       837         1      1          \n",
      "2      211         251       837         0      1          \n",
      "3      211         251       837         2      1     ::   \n",
      "4      211         251       837         1      1          \n",
      "\n",
      "                                          clean_text  \n",
      "0  Its just over 2 years since I was diagnosed wi...  \n",
      "1  Its Sunday I need a break so Im planning to sp...  \n",
      "2  Awake but tired I need to sleep but my brain h...  \n",
      "3  RT bears make perfect gifts and are great for ...  \n",
      "4  Its hard to say whether packing lists are maki...  \n"
     ]
    }
   ],
   "source": [
    "# define the function\n",
    "# apply the function to your dataframe\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # remove extra space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['post_text'].apply(clean_text)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44d9f5-2311-4379-ba10-bd9a4ba87c8e",
   "metadata": {},
   "source": [
    "## 5. Analysis 1 (Sentiment Analysis)\n",
    "\n",
    "Choose one analysis from (1)Sentiment Analysis, (2)N-grams and Phrase Analysis, (3)Collocation Analysis, (4)Part-of-Speech Tagging, (5)Named Entity Recognition, and (6)Dependency Parsing.\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ffee0b-2086-4828-8f12-1296c5d0c039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jovyan/.local/lib/python3.10/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: click in /home/jovyan/.local/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4622b356-7b22-4fd9-be29-d76d500b2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
       "1    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
       "2    {'neg': 0.259, 'neu': 0.741, 'pos': 0.0, 'comp...\n",
       "3    {'neg': 0.0, 'neu': 0.715, 'pos': 0.285, 'comp...\n",
       "4    {'neg': 0.06, 'neu': 0.819, 'pos': 0.121, 'com...\n",
       "Name: sentiment_scores, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sentiment_scores'] = df['post_text'].apply(lambda x: sia.polarity_scores(x))\n",
    "df['sentiment_scores'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6ee3d-825a-4ef0-9193-8d69e95b29ad",
   "metadata": {},
   "source": [
    "## 6. Analysis 2 (Named Entity Recognition)\n",
    "\n",
    "Choose another analysis from (1)Sentiment Analysis, (2)N-grams and Phrase Analysis, (3)Collocation Analysis, (4)Part-of-Speech Tagging, (5)Named Entity Recognition, and (6)Dependency Parsing.\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ee10ca7-baf9-4bfc-8d07-9b140366f5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (67.4.0)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (913 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.3/913.3 kB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.2)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/jovyan/.local/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 wasabi-1.1.1\n",
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/jovyan/.local/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70bdcad-f8c5-4cf5-9d26-b7d67cd42101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             post_id                    post_created  \\\n",
      "0           0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n",
      "1           1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n",
      "2           2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n",
      "3           3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n",
      "4           4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n",
      "\n",
      "                                           post_text     user_id  followers  \\\n",
      "0  It's just over 2 years since I was diagnosed w...  1013187241         84   \n",
      "1  It's Sunday, I need a break, so I'm planning t...  1013187241         84   \n",
      "2  Awake but tired. I need to sleep but my brain ...  1013187241         84   \n",
      "3  RT @SewHQ: #Retro bears make perfect gifts and...  1013187241         84   \n",
      "4  It’s hard to say whether packing lists are mak...  1013187241         84   \n",
      "\n",
      "   friends  favourites  statuses  retweets  label  \\\n",
      "0      211         251       837         0      1   \n",
      "1      211         251       837         1      1   \n",
      "2      211         251       837         0      1   \n",
      "3      211         251       837         2      1   \n",
      "4      211         251       837         1      1   \n",
      "\n",
      "                                          clean_text  \\\n",
      "0  Its just over 2 years since I was diagnosed wi...   \n",
      "1  Its Sunday I need a break so Im planning to sp...   \n",
      "2  Awake but tired I need to sleep but my brain h...   \n",
      "3  RT bears make perfect gifts and are great for ...   \n",
      "4  Its hard to say whether packing lists are maki...   \n",
      "\n",
      "                             entity  \n",
      "0  [(2 years, DATE), (Today, DATE)]  \n",
      "1                  [(Sunday, DATE)]  \n",
      "2                                []  \n",
      "3                [(Octobers, DATE)]  \n",
      "4                                []  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "df['clean_text'] = df['post_text'].apply(clean_text)\n",
    "text = df['clean_text']\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "df['entity'] = df['clean_text'].apply(named_entity_recognition)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2adeba-0c61-4216-9042-e79337da575c",
   "metadata": {},
   "source": [
    "## 7. Push Your Results to GitHub\n",
    "\n",
    "As you did in previous weeks:\n",
    "1. `git status`\n",
    "2. `git add`\n",
    "3. `git commit -m \"type your message here\"`\n",
    "4. `git push`\n",
    "\n",
    "If you can't push it to GitHub, it's okay to manually uploaded it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
